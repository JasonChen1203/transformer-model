July 3 - finish ScaledDotProductAttention
July 6 - finish MultiheadAttention
July 7 - set up tensorflow & required packages


Plans:
July 8 - finish Transformer Encoder
July 9 - finish Transformer Decoder

Implementing Input & Output Embedding + Positional Encoding:
 1. convert sentences into int vectors (ex. ["I am bad"] => [2 0 1])]
 2. using embedding, convert the int vectors into vectors of dimension d_model
 3. perform positional encoding - this part only needs the dimensions
 4. sum the embeddings & positional encoding


To Do:
 - decide whether is_masking should stay in EncoderLayer & Encoder
