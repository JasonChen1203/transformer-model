July 3 - finish ScaledDotProductAttention
July 6 - finish MultiheadAttention
July 7 - set up tensorflow & required packages
July 9 - finish Transformer Embedding, Transformer Encoder
July 10 - finish Transformer Decoder (with minor problems on the masked MultiheadAttention)'
July 13 - masking done


Plans:
July 14 - finish Transformer model
July 16 - finish training & evaluating code
July 18 - finish documentation


Implementing Input & Output Embedding + Positional Encoding:
 1. convert sentences into int vectors (ex. ["I am bad"] => [2 0 1])]
 2. using embedding, convert the int vectors into vectors of dimension d_model
 3. perform positional encoding - this part only needs the dimensions
 4. sum the embeddings & positional encoding


Masking:
 - Padding Mask
     - certain input sequences will first be zero-padded to a specific length, before feeding into the model => Zero Padding
     - purpose of padding mask is to ensure that the added zero values will not be processed by the model
     - padding mask marks the zero values by 1 (ex. [1, 2, 3, 4, 0, 0. 0] => [0, 0, 0, 0, 1, 1, 1])
 - Lookahead Mask
     - masking used in the 1st decoder sublayer
     - purpose of lookahead mask is to make sure that the succeeding words will not be considered in the model
     - lookahead mask marks the succeeding words by 1 
 - in the sample implementation, both the padding & lookahead mask are used in the masked MultiheadAttention



