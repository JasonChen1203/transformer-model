To-Do:
 - potential optimizations:
    - abandon sentences with seq_len > 100
    - resolve the issue with len(pred) always equal to len(target): potential sol is to add <EOL> symbol to input text
    - use logits & SparseCategoricalCrossentropy
 - improving performance:
    - dataset with more concentrated vocab
 - documentation on training & testing specs


Implementing Input & Output Embedding + Positional Encoding:
 1. convert sentences into int vectors (ex. ["I am bad"] => [2 0 1])]
 2. using embedding, convert the int vectors into vectors of dimension d_model
 3. perform positional encoding - this part only needs the dimensions
 4. sum the embeddings & positional encoding


Masking:
 - Padding Mask
     - certain input sequences will first be zero-padded to a specific length, before feeding into the model => Zero Padding
     - purpose of padding mask is to ensure that the added zero values will not be processed by the model
     - padding mask marks the zero values by 1 (ex. [1, 2, 3, 4, 0, 0. 0] => [0, 0, 0, 0, 1, 1, 1])
 - Lookahead Mask
     - masking used in the 1st decoder sublayer
     - purpose of lookahead mask is to make sure that the succeeding words will not be considered in the model
     - lookahead mask marks the succeeding words by 1 


Training the model:
 1. implement lrate to be used within the Adam Optimizer
 2. preprocess the WMT dataset
 3. implement accurcy & BLEU functions or find them in keras/tf
 4. implement the function that trains the model

