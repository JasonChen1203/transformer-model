July 3 - finish ScaledDotProductAttention
July 6 - finish MultiheadAttention
July 7 - set up tensorflow & required packages
July 9 - finish Transformer Embedding, Transformer Encoder
July 10 - finish Transformer Decoder (with minor problems on the masked MultiheadAttention)'
July 13 - masking done
July 14 - Transformer model done


To-do:
 1. figure out train test split & batching on the dataset
 2. employ label smoothing with epsilon = 0.1
 3. implement checkpoints and averaging the checkpoints
 4. nltk.translate.bleu & sentence_bleu
 

Implementing Input & Output Embedding + Positional Encoding:
 1. convert sentences into int vectors (ex. ["I am bad"] => [2 0 1])]
 2. using embedding, convert the int vectors into vectors of dimension d_model
 3. perform positional encoding - this part only needs the dimensions
 4. sum the embeddings & positional encoding


Masking:
 - Padding Mask
     - certain input sequences will first be zero-padded to a specific length, before feeding into the model => Zero Padding
     - purpose of padding mask is to ensure that the added zero values will not be processed by the model
     - padding mask marks the zero values by 1 (ex. [1, 2, 3, 4, 0, 0. 0] => [0, 0, 0, 0, 1, 1, 1])
 - Lookahead Mask
     - masking used in the 1st decoder sublayer
     - purpose of lookahead mask is to make sure that the succeeding words will not be considered in the model
     - lookahead mask marks the succeeding words by 1 
 - in the sample implementation, both the padding & lookahead mask are used in the masked MultiheadAttention


Training the model:
 1. implement lrate to be used within the Adam Optimizer
 2. preprocess the WMT dataset
 3. implement accurcy & BLEU functions or find them in keras/tf
 4. implement the function that trains the model

